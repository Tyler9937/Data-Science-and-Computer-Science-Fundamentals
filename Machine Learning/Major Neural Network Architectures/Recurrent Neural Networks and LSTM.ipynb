{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing needed libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.layers import Bidirectional, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Nerual Network with Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 128)               24704     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,994\n",
      "Trainable params: 89,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiaing the model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
    "model.add(layers.SimpleRNN(128))\n",
    "\n",
    "# Add an additional hidden layer\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "# View architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Nerual Network with Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               98816     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,106\n",
      "Trainable params: 164,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: https://keras.io/guides/working_with_rnns/\n",
    "\n",
    "# LSTM network example\n",
    "model = keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM NN with on Text Prediction**\n",
    "\n",
    "-  The novel is the Adventures of Sherlock Holmes by Arthur Conan Doyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-practice-datasets/main/unit_4/sherlock.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Strip the \\r\\n characters\n",
    "text = text.replace('\\r\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding the Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters in the text: 91\n"
     ]
    }
   ],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Find the unique characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "print('The number of unique characters in the text:', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Sequences to train on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  54974\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "# Encode the characters using the lookup tables\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "# Initialize empty lists to hold the sequences\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "# Loop through the entire text\n",
    "for i in range(0, len(encoded) - maxlen, step): \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "\n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\AppData\\Local\\Temp/ipykernel_6736/421023254.py:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
      "C:\\Users\\tyler\\AppData\\Local\\Temp/ipykernel_6736/421023254.py:7: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences so all are equal\n",
    "seq = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=40)\n",
    "\n",
    "# Create x and y\n",
    "# Create arrays of zeros (False)\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "# Turn on the location (set to True) when the character is present\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "\n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=64, input_dim=len(chars)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1718/1718 - 28s - loss: 2.5756 - 28s/epoch - 16ms/step\n",
      "Epoch 2/5\n",
      "1718/1718 - 26s - loss: 2.1857 - 26s/epoch - 15ms/step\n",
      "Epoch 3/5\n",
      "1718/1718 - 25s - loss: 2.0592 - 25s/epoch - 15ms/step\n",
      "Epoch 4/5\n",
      "1718/1718 - 25s - loss: 1.9748 - 25s/epoch - 15ms/step\n",
      "Epoch 5/5\n",
      "1718/1718 - 26s - loss: 1.9077 - 26s/epoch - 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24402974ee0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(seq, y, batch_size=32,\n",
    "          epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting outputs back to text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and convert text back into characters\n",
    "def generate_text(model, seed, length):\n",
    "\n",
    "  encoded = [char_int[c] for c in seed]\n",
    "\n",
    "  generated = ''\n",
    "  generated += seed\n",
    "  model.reset_states()\n",
    "\n",
    "  start_index = 0 \n",
    "\n",
    "  for _ in range(length):\n",
    "\n",
    "      sample = encoded[start_index:start_index+10]      \n",
    "      sample = np.array(sample)\n",
    "      sample = np.expand_dims(sample,0)\n",
    "\n",
    "      pred = model.predict(sample)\n",
    "      pred = tf.squeeze(pred, 0)\n",
    "      next_char = np.argmax(pred)\n",
    "      encoded.append(next_char)\n",
    "      generated += int_char[next_char]\n",
    "\n",
    "      start_index += 1\n",
    "\n",
    "  return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theoriesmeyeiloattn ts h sasenhiyoynt ye ah the r nedteeore af  tev aoyeyan trt nee af  te eng th then  totke ah terttthe r nd yd dnynn eethetashs   yn n y aoaa  ahe setot  rdn dtn tlhneo k    dwheotot t  tlhntxdetheshe  ssh h rh  thneeeee setoetoooooooosdd er y e aalooaoaoa k  mrr  shetrhtaeaotooethee  rsetllloaerr rhththheeoeien er edtaeeawaweawlhe r   daoetrd ur        eaxeaoaxs ddrntrrr mnaayeatlha  e'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed text which the model will use to generate the predicted text\n",
    "seed_text = \"I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theories\"\n",
    "\n",
    "generate_text(model, seed_text, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Increasing Training Epoches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with more epochs\n",
    "model.fit(seq, y, batch_size=32,\n",
    "          epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theories\"\n",
    "\n",
    "generate_text(model, seed_text, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theoriesdoiinwoktis asty fa-eeiclwegtrgssah bhe   nt.nrlfc-rrtdxoed GevccGsatrtin!y ing prlosa,IoIwoectiiocc.-ihIcpez bhe   cs,.nrrgin?hj—ffcr trmc!séBe  ,eoit-l suent  ew  E_ eTeoaiebmiL4aelay4ve:img” wseuWeoocet  t  t s onn”y”“j”]g i IeenoTTlJ ”   ana”,e”’oeeoIieaepaovP   kt HeCtrt  i xii vO‘zllr1mcsasg?b! ’’ e dn e hh lnhdnnr rs o  h eLcn.   Oa   rtt ddzt eeoIdT   ddc s snnnn”n£sJFsœe,aT e-Meee ioS s e'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b8e1b44fef6f769bdcd84daa9e6bcf329c54142ae85aa21700236386f9708da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
