{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import needed libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the locally saved file from the link above\n",
    "df_yelp = pd.read_csv('data/yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**split dataset**\n",
    "\n",
    "- doing before vectorization to avoid leaking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature and target variables\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# Train-test split\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<750x2864 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3051 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate and fit the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2))\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "# Vectorize the training and testing data\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "# Display the properties of the vectorized text\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline classification score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.588\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and fit a model\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a two step Pipline for cross fold validation**\n",
    "\n",
    "- Vectorizer\n",
    "- Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.611"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Pipeline\n",
    "pipe = Pipeline([('vect', vectorizer), # vectorizer\n",
    "                 ('clf', classifier) # classifier\n",
    "                ])\n",
    "\n",
    "# Define the parameter space for the grid serach\n",
    "parameters = {'clf__C': [1, 10, 1000000]} # C: regularization strength\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(sentences, y);\n",
    "\n",
    "# Print out the best score\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipline with random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and fit a model\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.591"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 10)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 4]\n",
    "Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "\n",
    "# Define the Pipeline\n",
    "pipe = Pipeline([('vect', vectorizer), # vectorizer\n",
    "                 ('clf', classifier) # classifier\n",
    "                ])\n",
    "\n",
    "# Define the parameter space for the grid serach\n",
    "# you need the 'clf' name followed by two __ then the parameter 'n_estimators'\n",
    "parameters = {'clf__n_estimators': n_estimators,\n",
    "              'clf__max_features': max_features,\n",
    "              'clf__max_depth': max_depth,\n",
    "              'clf__min_samples_split': min_samples_split,\n",
    "              'clf__min_samples_leaf': min_samples_leaf,\n",
    "              'clf__bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(sentences, y);\n",
    "\n",
    "# Print out the best score\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Singular Value Decomposition (SVD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = pd.read_csv('data/yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()\n",
    "\n",
    "# Create the features and target\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# Instantiate the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2))\n",
    "\n",
    "# Instantiate the classifier (defaults)\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Instantiate the LSA (SVD) algorithm (defaults)\n",
    "svd = TruncatedSVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5999999999999999"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSA part\n",
    "lsa = Pipeline([('vect', vectorizer), ('svd', svd)])\n",
    "\n",
    "# Combine into one pipeline\n",
    "pipe = Pipeline([('lsa', lsa), ('clf', classifier)])\n",
    "\n",
    "# Define the parameter space for the grid search\n",
    "parameters = {\n",
    "    'lsa__svd__n_components': (100,250),\n",
    "    'lsa__vect__max_df': (0.9, 1.0), # max document frequency\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(sentences, y);\n",
    "\n",
    "# Display the best score from the grid-search\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVD with amazon dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.643"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp = pd.read_csv('data/amazon_cells_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()\n",
    "\n",
    "# Create the features and target\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# Instantiate the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2))\n",
    "\n",
    "# Instantiate the classifier (defaults)\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Instantiate the LSA (SVD) algorithm (defaults)\n",
    "svd = TruncatedSVD()\n",
    "\n",
    "# LSA part\n",
    "lsa = Pipeline([('vect', vectorizer), ('svd', svd)])\n",
    "\n",
    "# Combine into one pipeline\n",
    "pipe = Pipeline([('lsa', lsa), ('clf', classifier)])\n",
    "\n",
    "# Define the parameter space for the grid search\n",
    "parameters = {\n",
    "    'lsa__svd__n_components': (100,250),\n",
    "    'lsa__vect__max_df': (0.9, 1.0), # max document frequency\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(sentences, y);\n",
    "\n",
    "# Display the best score from the grid-search\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline spacy word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy including word embeddings:  0.856\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Read in the locally saved file from UCI website\n",
    "df_yelp = pd.read_csv('data/yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()\n",
    "\n",
    "# Create the features and target\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# Train-test split\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Function to return the vector for each sentence in a document\n",
    "def get_word_vectors(docs):\n",
    "    return [nlp(doc).vector for doc in docs]\n",
    "\n",
    "# Get the vectors for each sentence (mean of all the word vectors)\n",
    "X_train = get_word_vectors(sentences_train)\n",
    "X_test = get_word_vectors(sentences_test)\n",
    "\n",
    "# Instantiate the classifier (defaults)\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "# Print out the accuracy score\n",
    "print(\"Accuracy including word embeddings: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**amazon data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy including word embeddings:  0.86\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Read in the locally saved file from UCI website\n",
    "df_yelp = pd.read_csv('data/amazon_cells_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()\n",
    "\n",
    "# Create the features and target\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# Train-test split\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Function to return the vector for each sentence in a document\n",
    "def get_word_vectors(docs):\n",
    "    return [nlp(doc).vector for doc in docs]\n",
    "\n",
    "# Get the vectors for each sentence (mean of all the word vectors)\n",
    "X_train = get_word_vectors(sentences_train)\n",
    "X_test = get_word_vectors(sentences_test)\n",
    "\n",
    "# Instantiate the classifier (defaults)\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "# Print out the accuracy score\n",
    "print(\"Accuracy including word embeddings: \", score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b8e1b44fef6f769bdcd84daa9e6bcf329c54142ae85aa21700236386f9708da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
